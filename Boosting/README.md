# Boosting for Classification

This notebook contains the full implementation and analysis of boosting algorithms, as part of Homework 2 in the course. The assignment provides a hands-on introduction to the principles and performance of ensemble learning methods, with a focus on gradient boosting and AdaBoost.

---

## Overview

Key components of this exercise include:

- Understanding and implementing boosting algorithms from scratch
- Constructing and evaluating weak learners
- Iterative boosting and weight updates
- Visualizing error metrics and decision boundaries
- Analyzing the bias-variance tradeoff in ensemble learning

All tasks are designed to deepen your understanding of ensemble methods, particularly in the context of classification problems.

---

## Submission Guidelines

- Only the notebook file (`HW2_315856484_209209691.ipynb`) should be submitted.
- Do **not** include zipped folders, external scripts, or additional files.
- Follow the notebook's structure precisely and complete all required tasks.

---

## Technical Requirements

- Ensure your implementation is:
  - Correct and complete
  - Efficient and **vectorized** (non-vectorized code will receive point deductions)
- Tests are encouraged for verification but are not graded
- You may use libraries such as:
  - `numpy`
  - `matplotlib`
  - `sklearn` (where permitted)

---

## Environment Setup

Install the required packages using:

```bash
pip install numpy matplotlib scikit-learn
